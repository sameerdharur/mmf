task_attributes:
    vqa:
        datasets:
        - vqa_introspect
        #- vqa2
        dataset_size_proportional_sampling: true
        dataset_attributes:
            vqa_introspect:
                data_root_dir: ../data
                image_depth_first: false
                fast_read: false
                image_features:
                    train:
                    - coco/detectron_fix_100/fc6/train_val_2014,coco/resnet152/train_val_2014
                    - coco/detectron_fix_100/fc6/train_val_2014,coco/resnet152/train_val_2014
                    val:
                    - coco/detectron_fix_100/fc6/train_val_2014,coco/resnet152/train_val_2014
                    test:
                    - coco/detectron_fix_100/fc6/train_val_2014,coco/resnet152/train_val_2014
                imdb_files:
                    train:
                    #- /srv/share/ram/projects/squint_cvpr_oral/code/pythia/data/imdb/vqa/v1_imdb_train2014.npy
                    - /srv/share/sameer/pythia/data/nlp_project/datasets/train_binary.npy
                    #- /srv/share/sameer/pythia/data/nlp_project/datasets/sub_other_questions_train_clean.npy
                    - imdb/vqa/imdb_train2014.npy
                    val:
                    #- /srv/share/sameer/pythia_results/small_datasets/split_9.npy
                    #- /srv/share/sameer/pythia/data/nlp_project/datasets/val_v2.npy
                    #- /coc/pskynet1/ram/projects/squint_cvpr_oral/code/pythia/data/imdb/vqa/imdb_vqa_introspect1.0_val.npy
                    - /srv/share/sameer/pythia/data/nlp_project/datasets/small_datasets/set_1.npy
                    #- /srv/share/sameer/pythia/data/nlp_project/datasets/sub_other_questions_train_mini.npy
                    #- /srv/share/sameer/pythia/data/nlp_project/datasets/val_sub_other_questions.npy
                    #- /srv/share/sameer/pythia/data/nlp_project/datasets/reasoning_questions_val_split.npy
                    #- /coc/pskynet1/ram/projects/squint_cvpr_oral/code/pythia/data/imdb/vqa/imdb_vqa2014_val_reasoning_questions.npy
                    #- /srv/share/ram/projects/squint_cvpr_oral/code/pythia/data/imdb/vqa/imdb_val2014.npy
                    #- /srv/share/purva/Assignments/nlp/project/data/final_val.npy
                    #- /srv/share/purva/Assignments/nlp/project/data/final_train.npy
                    test:
                    - /srv/share/sameer/pythia/data/nlp_project/datasets/val_v2.npy
                    #- /srv/share/sameer/pythia/test_mini.npy
                features_max_len: 100
                processors:
                  text_processor:
                    type: vocab
                    params:
                      max_length: 14
                      vocab:
                        type: intersected
                        embedding_name: glove.6B.300d
                        vocab_file: vocabs/vocabulary_100k.txt
                      preprocessor:
                        type: simple_sentence
                        params: {}
                  answer_processor:
                    type: vqa_answer
                    params:
                      num_answers: 10
                      vocab_file: vocabs/answers_vqa.txt
                      preprocessor:
                        type: simple_word
                        params: {}
                  context_processor:
                    type: fasttext
                    params:
                      max_length: 50
                      model_file: .vector_cache/wiki.en.bin
                  ocr_token_processor:
                    type: simple_word
                    params: {}
                  bbox_processor:
                    type: bbox
                    params:
                      max_length: 50
                return_info: true
                # Return OCR information
                use_ocr: false
                # Return spatial information of OCR tokens if present
                use_ocr_info: false
training_parameters:
    monitored_metric: ranking_accuracy
    metric_minimize: false